# -*- coding: utf-8 -*-
"""Sentiment Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I7vPO1_bdvL7IgXJoWxy0p-jN413Wsj7
"""

# Importing Libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('GBcomments.csv',nrows=30000)

# Preprocessing for Stemming and Stopward Removal

import re                                                                # re: For regex-based text cleaning.
import nltk                                                              # nltk: Natural Language Toolkit, for NLP tasks.
nltk.download('stopwords')                   # stopwords: Common words like “the”, “is” which are removed to focus on significant words.
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer                           # PorterStemmer: Reduces words to their root form.
corpus = []                                                          # corpus: List to store cleaned text data.

for i in range(0, 1000):
 review = re.sub('[^a-zA-Z]', ' ', dataset[dataset.columns[0]][i])
 review = review.lower()
 review = review.split()

ps = PorterStemmer()                                       # Initializes the stemmer.
all_stopwords = stopwords.words('english')                 # Loads English stopwords and retains "not" to preserve sentiment context.
all_stopwords.remove('not')

review = [ps.stem(word) for word in review if not word in set(all_stopwords)]            # Applies stemming and removes stopwords.
review = ' '.join(review)                                                         # Joins processed words into a single string.
corpus.append(review)                                     # Adds it to the corpus.

# Feature Extraction using CountVectorizer

from sklearn.feature_extraction.text import CountVectorizer                  # Converts the text to a numerical feature vector.
cv = CountVectorizer(max_features = 1500)                           # Only considers the top 1500 frequent words.
X = cv.fit_transform(corpus).toarray()                       # X: Features; y: Labels from the last column of the dataset.
y = dataset.iloc[:, -1].values

# Rebuilding Corpus with Adjusted Data Handling

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split


# Ensure corpus and target have the same length                          # Extracts all comment texts.Transforms them into a bag-of-words model.
corpus = dataset['comment_text'].astype(str).tolist()  # Adjust column name
cv = CountVectorizer(max_features=1500)
X = cv.fit_transform(corpus).toarray()

# Ensure equal size
num_samples = min(len(X), len(dataset))               # Ensures X and y have equal lengths.
X = X[:num_samples]
y = dataset.iloc[:num_samples, -1].values

# Splitting Data into Train and Test Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
                                                                     # Splits 80% data for training and 20% for testing.
print(f"Training Samples: {len(X_train)}, Test Samples: {len(X_test)}")

# Import and Train Multiple ML Models

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer


# Standardizes features to improve model performance (except Naive Bayes & Tree-based models).
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# Training Models
gnb_classifier = GaussianNB()                 # Trains Naive Bayes classifier.
gnb_classifier.fit(X_train, y_train)


# Trains a linear SVM on a smaller resampled dataset (for speed).
from sklearn.utils import resample
svc_classifier = SVC(kernel='linear', random_state=0)
X_train_small, y_train_small = resample(X_train_scaled, y_train, n_samples=5000, random_state=42)
svc_classifier.fit(X_train_small, y_train_small)


# Trains Logistic Regression on same small set.
from sklearn.linear_model import LogisticRegression
from sklearn.utils import resample

logreg_classifier = LogisticRegression(random_state=0, solver='saga', max_iter=300)

X_train_small, y_train_small = resample(X_train_scaled, y_train, n_samples=5000, random_state=42)

logreg_classifier.fit(X_train_small, y_train_small)

print("Model trained successfully on reduced data!")


# Trains KNN with Euclidean distance.
knn_classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
knn_classifier.fit(X_train_scaled, y_train)


# Trains Decision Tree with entropy-based split.
dt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)
dt_classifier.fit(X_train, y_train)


# Trains Random Forest with 10 trees in parallel.
rf_classifier = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0, n_jobs=-1)
rf_classifier.fit(X_train, y_train)

print("All models trained successfully!")

# Model Evaluation

from sklearn.metrics import confusion_matrix, accuracy_score               # For performance metrics.

# List of models with their names
models = [                                                                      # Stores all classifiers for batch evaluation.
    ("Gaussian Naive Bayes", gnb_classifier, X_test),
    ("Support Vector Classifier (SVM)", svc_classifier, X_test_scaled),
    ("Logistic Regression", logreg_classifier, X_test_scaled),
    ("K-Nearest Neighbors (KNN)", knn_classifier, X_test_scaled),
    ("Decision Tree", dt_classifier, X_test),
    ("Random Forest", rf_classifier, X_test)
]

# Evaluate each model
for name, model, X_test_input in models:
    print(f"Evaluating {name}...")
    y_pred = model.predict(X_test_input)
    cm = confusion_matrix(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred) * 100

    print(f"Accuracy Score: {accuracy:.2f}%\n")

# Sentiment Analysis using TextBlob
!pip install textblob
from textblob import TextBlob

# Function to calculate polarity and determine sentiment
def get_sentiment(text):
    analysis = TextBlob(str(text))
    polarity = analysis.sentiment.polarity

    if polarity > 0:
        return 'Positive'
    elif polarity < 0:
        return 'Negative'
    else:
        return 'Neutral'


# Apply sentiment analysis to each comment
dataset['sentiment'] = dataset['comment_text'].apply(get_sentiment)

# Display the results
print(dataset[['comment_text', 'sentiment']].head())

# Word Cloud for Positive Comments

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
import nltk

# Download stopwords if not available
nltk.download('stopwords')

# Filter only positive comments
positive_comments = dataset[dataset['sentiment'] == 'Positive']

# Combine all positive comments into one string
total_comments = ' '.join(positive_comments['comment_text'].dropna())

# Remove stopwords
stop_words = set(stopwords.words('english'))

# Generate Word Cloud
wordcloud = WordCloud(width=1000, height=500, stopwords=stop_words, background_color='white').generate(total_comments)

# Plot the Word Cloud
plt.figure(figsize=(15, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Positive Comments')
plt.show()